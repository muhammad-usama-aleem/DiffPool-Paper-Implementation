{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Diffpool.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOXbuR7tn4kKn4ISutH/LF1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muhammad-usama-aleem/DiffPool-Paper-Implementation/blob/main/Diffpool.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVjqmHpFmV29",
        "outputId": "354dedb1-b781-4390-bc95-08e1d9aee95c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 7.9 MB 4.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 4.3 MB/s \n",
            "\u001b[?25h  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os.path as osp\n",
        "from math import ceil\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.datasets import TUDataset\n",
        "from torch_geometric.loader import DenseDataLoader\n",
        "from torch_geometric.nn import DenseSAGEConv, dense_diff_pool\n",
        "\n",
        "max_nodes = 150\n",
        "\n",
        "\n",
        "class MyFilter(object):\n",
        "    def __call__(self, data):\n",
        "        return data.num_nodes <= max_nodes\n",
        "\n",
        "\n",
        "dataset = TUDataset(root='data/PROTEINS_dense', name='PROTEINS', transform=T.ToDense(max_nodes),\n",
        "                    pre_filter=MyFilter())\n",
        "dataset = dataset.shuffle()\n",
        "n = (len(dataset) + 9) // 10\n",
        "test_dataset = dataset[:n]\n",
        "val_dataset = dataset[n:2 * n]\n",
        "train_dataset = dataset[2 * n:]\n",
        "test_loader = DenseDataLoader(test_dataset, batch_size=20)\n",
        "val_loader = DenseDataLoader(val_dataset, batch_size=20)\n",
        "train_loader = DenseDataLoader(train_dataset, batch_size=20)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzn9uPKRnJMi",
        "outputId": "2b620513-1e0d-42ea-b586-4d00d7bd4c02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://www.chrsmrrs.com/graphkerneldatasets/PROTEINS.zip\n",
            "Extracting data/PROTEINS_dense/PROTEINS/PROTEINS.zip\n",
            "Processing...\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get some basic info about the dataset\n",
        "print(f'Number of graphs: {len(dataset)}')\n",
        "print(f'Number of features: {dataset.num_features}')\n",
        "print(f'Number of classes: {dataset.num_classes}')\n",
        "print(50*'=')\n",
        "\n",
        "# There is only one graph in the dataset, use it as new data object\n",
        "data = dataset[0]  \n",
        "\n",
        "# Gather some statistics about the graph.\n",
        "print(data)\n",
        "print(f'Number of nodes: {data.num_nodes}')\n",
        "print(f'Number of edges: {data.num_edges}')\n",
        "# print(f'Number of training nodes: {data.train_mask.sum()}')\n",
        "# print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.2f}')\n",
        "# print(f'Is undirected: {data.is_undirected()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JOZFAH-KXCr",
        "outputId": "b9f280d0-7fb7-43e8-b003-a803ed7bb76d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of graphs: 1085\n",
            "Number of features: 3\n",
            "Number of classes: 2\n",
            "==================================================\n",
            "Data(x=[150, 3], y=[1], adj=[150, 150], mask=[150])\n",
            "Number of nodes: 150\n",
            "Number of edges: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GNN(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels,\n",
        "                 normalize=False, lin=True):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = DenseSAGEConv(in_channels, hidden_channels, normalize)\n",
        "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n",
        "        self.conv2 = DenseSAGEConv(hidden_channels, hidden_channels, normalize)\n",
        "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n",
        "        self.conv3 = DenseSAGEConv(hidden_channels, out_channels, normalize)\n",
        "        self.bn3 = torch.nn.BatchNorm1d(out_channels)\n",
        "\n",
        "        if lin is True:\n",
        "            self.lin = torch.nn.Linear(2 * hidden_channels + out_channels,\n",
        "                                       out_channels)\n",
        "        else:\n",
        "            self.lin = None\n",
        "\n",
        "    def bn(self, i, x):\n",
        "        batch_size, num_nodes, num_channels = x.size()\n",
        "\n",
        "        x = x.view(-1, num_channels)\n",
        "        x = getattr(self, f'bn{i}')(x)\n",
        "        x = x.view(batch_size, num_nodes, num_channels)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x, adj, mask=None):\n",
        "        batch_size, num_nodes, in_channels = x.size()\n",
        "\n",
        "        x0 = x\n",
        "        x1 = self.bn(1, self.conv1(x0, adj, mask).relu())\n",
        "        x2 = self.bn(2, self.conv2(x1, adj, mask).relu())\n",
        "        x3 = self.bn(3, self.conv3(x2, adj, mask).relu())\n",
        "\n",
        "        x = torch.cat([x1, x2, x3], dim=-1)\n",
        "\n",
        "        if self.lin is not None:\n",
        "            x = self.lin(x).relu()\n",
        "\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "wsx8APFNoZOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        num_nodes = ceil(0.25 * max_nodes)\n",
        "        self.gnn1_pool = GNN(dataset.num_features, 64, num_nodes)\n",
        "        self.gnn1_embed = GNN(dataset.num_features, 64, 64, lin=False)\n",
        "\n",
        "        num_nodes = ceil(0.25 * num_nodes)\n",
        "        self.gnn2_pool = GNN(3 * 64, 64, num_nodes)\n",
        "        self.gnn2_embed = GNN(3 * 64, 64, 64, lin=False)\n",
        "\n",
        "        self.gnn3_embed = GNN(3 * 64, 64, 64, lin=False)\n",
        "\n",
        "        self.lin1 = torch.nn.Linear(3 * 64, 64)\n",
        "        self.lin2 = torch.nn.Linear(64, dataset.num_classes)\n",
        "\n",
        "    def forward(self, x, adj, mask=None):\n",
        "        s = self.gnn1_pool(x, adj, mask)\n",
        "        x = self.gnn1_embed(x, adj, mask)\n",
        "\n",
        "        x, adj, l1, e1 = dense_diff_pool(x, adj, s, mask)\n",
        "\n",
        "        s = self.gnn2_pool(x, adj)\n",
        "        x = self.gnn2_embed(x, adj)\n",
        "\n",
        "        x, adj, l2, e2 = dense_diff_pool(x, adj, s)\n",
        "\n",
        "        x = self.gnn3_embed(x, adj)\n",
        "\n",
        "        x = x.mean(dim=1)\n",
        "        x = self.lin1(x).relu()\n",
        "        x = self.lin2(x)\n",
        "        return F.log_softmax(x, dim=-1), l1 + l2, e1 + e2\n"
      ],
      "metadata": {
        "id": "sTW8m6CxodWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Net().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "id": "-hYO4MWdrLa9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch):\n",
        "    model.train()\n",
        "    loss_all = 0\n",
        "\n",
        "    for data in train_loader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output, _, _ = model(data.x, data.adj, data.mask)\n",
        "        loss = F.nll_loss(output, data.y.view(-1))\n",
        "        loss.backward()\n",
        "        loss_all += data.y.size(0) * float(loss)\n",
        "        optimizer.step()\n",
        "    return loss_all / len(train_dataset)"
      ],
      "metadata": {
        "id": "dFUUI9hVrRNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "\n",
        "    for data in loader:\n",
        "        data = data.to(device)\n",
        "        pred = model(data.x, data.adj, data.mask)[0].max(dim=1)[1]\n",
        "        correct += int(pred.eq(data.y.view(-1)).sum())\n",
        "    return correct / len(loader.dataset)"
      ],
      "metadata": {
        "id": "qzbHWrZLrTbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_val_acc = test_acc = 0\n",
        "for epoch in range(1, 151):\n",
        "    train_loss = train(epoch)\n",
        "    val_acc = test(val_loader)\n",
        "    if val_acc > best_val_acc:\n",
        "        test_acc = test(test_loader)\n",
        "        best_val_acc = val_acc\n",
        "    print(f'Epoch: {epoch:03d}, Train Loss: {train_loss:.4f}, '\n",
        "          f'Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMmzyuBErVrW",
        "outputId": "b934d1ee-bdab-4726-f584-915fd7769164"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Train Loss: 0.5740, Val Acc: 0.6330, Test Acc: 0.6697\n",
            "Epoch: 002, Train Loss: 0.5429, Val Acc: 0.6789, Test Acc: 0.6606\n",
            "Epoch: 003, Train Loss: 0.5263, Val Acc: 0.6972, Test Acc: 0.6606\n",
            "Epoch: 004, Train Loss: 0.5178, Val Acc: 0.7248, Test Acc: 0.7064\n",
            "Epoch: 005, Train Loss: 0.5022, Val Acc: 0.6881, Test Acc: 0.7064\n",
            "Epoch: 006, Train Loss: 0.4914, Val Acc: 0.6697, Test Acc: 0.7064\n",
            "Epoch: 007, Train Loss: 0.4852, Val Acc: 0.6789, Test Acc: 0.7064\n",
            "Epoch: 008, Train Loss: 0.4688, Val Acc: 0.6147, Test Acc: 0.7064\n",
            "Epoch: 009, Train Loss: 0.4605, Val Acc: 0.6697, Test Acc: 0.7064\n",
            "Epoch: 010, Train Loss: 0.4635, Val Acc: 0.6881, Test Acc: 0.7064\n",
            "Epoch: 011, Train Loss: 0.4546, Val Acc: 0.7156, Test Acc: 0.7064\n",
            "Epoch: 012, Train Loss: 0.4218, Val Acc: 0.6606, Test Acc: 0.7064\n",
            "Epoch: 013, Train Loss: 0.4227, Val Acc: 0.6697, Test Acc: 0.7064\n",
            "Epoch: 014, Train Loss: 0.4289, Val Acc: 0.6972, Test Acc: 0.7064\n",
            "Epoch: 015, Train Loss: 0.4195, Val Acc: 0.6330, Test Acc: 0.7064\n",
            "Epoch: 016, Train Loss: 0.4293, Val Acc: 0.6789, Test Acc: 0.7064\n",
            "Epoch: 017, Train Loss: 0.4274, Val Acc: 0.6422, Test Acc: 0.7064\n",
            "Epoch: 018, Train Loss: 0.3714, Val Acc: 0.6330, Test Acc: 0.7064\n",
            "Epoch: 019, Train Loss: 0.3868, Val Acc: 0.6147, Test Acc: 0.7064\n",
            "Epoch: 020, Train Loss: 0.3440, Val Acc: 0.6239, Test Acc: 0.7064\n",
            "Epoch: 021, Train Loss: 0.3515, Val Acc: 0.6055, Test Acc: 0.7064\n",
            "Epoch: 022, Train Loss: 0.3260, Val Acc: 0.5872, Test Acc: 0.7064\n",
            "Epoch: 023, Train Loss: 0.3298, Val Acc: 0.6239, Test Acc: 0.7064\n",
            "Epoch: 024, Train Loss: 0.3963, Val Acc: 0.6055, Test Acc: 0.7064\n",
            "Epoch: 025, Train Loss: 0.3740, Val Acc: 0.6789, Test Acc: 0.7064\n",
            "Epoch: 026, Train Loss: 0.3323, Val Acc: 0.6147, Test Acc: 0.7064\n",
            "Epoch: 027, Train Loss: 0.2720, Val Acc: 0.5596, Test Acc: 0.7064\n",
            "Epoch: 028, Train Loss: 0.2818, Val Acc: 0.5596, Test Acc: 0.7064\n",
            "Epoch: 029, Train Loss: 0.2594, Val Acc: 0.5505, Test Acc: 0.7064\n",
            "Epoch: 030, Train Loss: 0.2420, Val Acc: 0.5780, Test Acc: 0.7064\n",
            "Epoch: 031, Train Loss: 0.3272, Val Acc: 0.5505, Test Acc: 0.7064\n",
            "Epoch: 032, Train Loss: 0.3138, Val Acc: 0.6055, Test Acc: 0.7064\n",
            "Epoch: 033, Train Loss: 0.2413, Val Acc: 0.6239, Test Acc: 0.7064\n",
            "Epoch: 034, Train Loss: 0.2237, Val Acc: 0.5505, Test Acc: 0.7064\n",
            "Epoch: 035, Train Loss: 0.3239, Val Acc: 0.5505, Test Acc: 0.7064\n",
            "Epoch: 036, Train Loss: 0.2498, Val Acc: 0.6422, Test Acc: 0.7064\n",
            "Epoch: 037, Train Loss: 0.2158, Val Acc: 0.5872, Test Acc: 0.7064\n",
            "Epoch: 038, Train Loss: 0.2187, Val Acc: 0.6514, Test Acc: 0.7064\n",
            "Epoch: 039, Train Loss: 0.1775, Val Acc: 0.6697, Test Acc: 0.7064\n",
            "Epoch: 040, Train Loss: 0.1607, Val Acc: 0.6422, Test Acc: 0.7064\n",
            "Epoch: 041, Train Loss: 0.2286, Val Acc: 0.6330, Test Acc: 0.7064\n",
            "Epoch: 042, Train Loss: 0.1960, Val Acc: 0.6422, Test Acc: 0.7064\n",
            "Epoch: 043, Train Loss: 0.1282, Val Acc: 0.6606, Test Acc: 0.7064\n",
            "Epoch: 044, Train Loss: 0.1551, Val Acc: 0.6606, Test Acc: 0.7064\n",
            "Epoch: 045, Train Loss: 0.1736, Val Acc: 0.6881, Test Acc: 0.7064\n",
            "Epoch: 046, Train Loss: 0.2198, Val Acc: 0.6330, Test Acc: 0.7064\n",
            "Epoch: 047, Train Loss: 0.1045, Val Acc: 0.6147, Test Acc: 0.7064\n",
            "Epoch: 048, Train Loss: 0.0857, Val Acc: 0.6514, Test Acc: 0.7064\n",
            "Epoch: 049, Train Loss: 0.0943, Val Acc: 0.6330, Test Acc: 0.7064\n",
            "Epoch: 050, Train Loss: 0.1388, Val Acc: 0.6239, Test Acc: 0.7064\n",
            "Epoch: 051, Train Loss: 0.1254, Val Acc: 0.6147, Test Acc: 0.7064\n",
            "Epoch: 052, Train Loss: 0.1154, Val Acc: 0.6330, Test Acc: 0.7064\n",
            "Epoch: 053, Train Loss: 0.0934, Val Acc: 0.6147, Test Acc: 0.7064\n",
            "Epoch: 054, Train Loss: 0.0719, Val Acc: 0.6147, Test Acc: 0.7064\n",
            "Epoch: 055, Train Loss: 0.1076, Val Acc: 0.6147, Test Acc: 0.7064\n",
            "Epoch: 056, Train Loss: 0.0927, Val Acc: 0.5963, Test Acc: 0.7064\n",
            "Epoch: 057, Train Loss: 0.1085, Val Acc: 0.6055, Test Acc: 0.7064\n",
            "Epoch: 058, Train Loss: 0.0832, Val Acc: 0.6789, Test Acc: 0.7064\n",
            "Epoch: 059, Train Loss: 0.1026, Val Acc: 0.6147, Test Acc: 0.7064\n",
            "Epoch: 060, Train Loss: 0.0764, Val Acc: 0.5505, Test Acc: 0.7064\n",
            "Epoch: 061, Train Loss: 0.0993, Val Acc: 0.6147, Test Acc: 0.7064\n",
            "Epoch: 062, Train Loss: 0.0887, Val Acc: 0.5872, Test Acc: 0.7064\n",
            "Epoch: 063, Train Loss: 0.0512, Val Acc: 0.5963, Test Acc: 0.7064\n",
            "Epoch: 064, Train Loss: 0.0945, Val Acc: 0.5872, Test Acc: 0.7064\n",
            "Epoch: 065, Train Loss: 0.0694, Val Acc: 0.6514, Test Acc: 0.7064\n",
            "Epoch: 066, Train Loss: 0.0840, Val Acc: 0.6055, Test Acc: 0.7064\n",
            "Epoch: 067, Train Loss: 0.0823, Val Acc: 0.5505, Test Acc: 0.7064\n",
            "Epoch: 068, Train Loss: 0.0527, Val Acc: 0.6422, Test Acc: 0.7064\n",
            "Epoch: 069, Train Loss: 0.0196, Val Acc: 0.6422, Test Acc: 0.7064\n",
            "Epoch: 070, Train Loss: 0.0373, Val Acc: 0.5963, Test Acc: 0.7064\n",
            "Epoch: 071, Train Loss: 0.0376, Val Acc: 0.6147, Test Acc: 0.7064\n",
            "Epoch: 072, Train Loss: 0.0268, Val Acc: 0.6055, Test Acc: 0.7064\n",
            "Epoch: 073, Train Loss: 0.0194, Val Acc: 0.6514, Test Acc: 0.7064\n",
            "Epoch: 074, Train Loss: 0.0175, Val Acc: 0.6055, Test Acc: 0.7064\n",
            "Epoch: 075, Train Loss: 0.0188, Val Acc: 0.5413, Test Acc: 0.7064\n",
            "Epoch: 076, Train Loss: 0.0088, Val Acc: 0.6055, Test Acc: 0.7064\n",
            "Epoch: 077, Train Loss: 0.0069, Val Acc: 0.5229, Test Acc: 0.7064\n",
            "Epoch: 078, Train Loss: 0.0088, Val Acc: 0.6147, Test Acc: 0.7064\n",
            "Epoch: 079, Train Loss: 0.0071, Val Acc: 0.5780, Test Acc: 0.7064\n",
            "Epoch: 080, Train Loss: 0.0235, Val Acc: 0.5872, Test Acc: 0.7064\n",
            "Epoch: 081, Train Loss: 0.0300, Val Acc: 0.5872, Test Acc: 0.7064\n",
            "Epoch: 082, Train Loss: 0.1124, Val Acc: 0.5138, Test Acc: 0.7064\n",
            "Epoch: 083, Train Loss: 0.1774, Val Acc: 0.6239, Test Acc: 0.7064\n",
            "Epoch: 084, Train Loss: 0.1946, Val Acc: 0.5413, Test Acc: 0.7064\n",
            "Epoch: 085, Train Loss: 0.1318, Val Acc: 0.5229, Test Acc: 0.7064\n",
            "Epoch: 086, Train Loss: 0.0737, Val Acc: 0.5872, Test Acc: 0.7064\n",
            "Epoch: 087, Train Loss: 0.0447, Val Acc: 0.5780, Test Acc: 0.7064\n",
            "Epoch: 088, Train Loss: 0.0247, Val Acc: 0.6239, Test Acc: 0.7064\n",
            "Epoch: 089, Train Loss: 0.0158, Val Acc: 0.6147, Test Acc: 0.7064\n",
            "Epoch: 090, Train Loss: 0.0102, Val Acc: 0.6239, Test Acc: 0.7064\n",
            "Epoch: 091, Train Loss: 0.0080, Val Acc: 0.6330, Test Acc: 0.7064\n",
            "Epoch: 092, Train Loss: 0.0094, Val Acc: 0.6055, Test Acc: 0.7064\n",
            "Epoch: 093, Train Loss: 0.0055, Val Acc: 0.6147, Test Acc: 0.7064\n",
            "Epoch: 094, Train Loss: 0.0067, Val Acc: 0.5872, Test Acc: 0.7064\n",
            "Epoch: 095, Train Loss: 0.0051, Val Acc: 0.6147, Test Acc: 0.7064\n",
            "Epoch: 096, Train Loss: 0.0046, Val Acc: 0.6147, Test Acc: 0.7064\n",
            "Epoch: 097, Train Loss: 0.0059, Val Acc: 0.6147, Test Acc: 0.7064\n",
            "Epoch: 098, Train Loss: 0.0048, Val Acc: 0.6147, Test Acc: 0.7064\n",
            "Epoch: 099, Train Loss: 0.0037, Val Acc: 0.6147, Test Acc: 0.7064\n",
            "Epoch: 100, Train Loss: 0.0033, Val Acc: 0.6147, Test Acc: 0.7064\n",
            "Epoch: 101, Train Loss: 0.0031, Val Acc: 0.6147, Test Acc: 0.7064\n",
            "Epoch: 102, Train Loss: 0.0030, Val Acc: 0.6147, Test Acc: 0.7064\n",
            "Epoch: 103, Train Loss: 0.0029, Val Acc: 0.6055, Test Acc: 0.7064\n",
            "Epoch: 104, Train Loss: 0.0031, Val Acc: 0.6239, Test Acc: 0.7064\n",
            "Epoch: 105, Train Loss: 0.0036, Val Acc: 0.5780, Test Acc: 0.7064\n",
            "Epoch: 106, Train Loss: 0.0043, Val Acc: 0.6239, Test Acc: 0.7064\n",
            "Epoch: 107, Train Loss: 0.0035, Val Acc: 0.6055, Test Acc: 0.7064\n",
            "Epoch: 108, Train Loss: 0.0034, Val Acc: 0.6147, Test Acc: 0.7064\n",
            "Epoch: 109, Train Loss: 0.0029, Val Acc: 0.6147, Test Acc: 0.7064\n",
            "Epoch: 110, Train Loss: 0.0028, Val Acc: 0.6239, Test Acc: 0.7064\n",
            "Epoch: 111, Train Loss: 0.0029, Val Acc: 0.5963, Test Acc: 0.7064\n",
            "Epoch: 112, Train Loss: 0.0035, Val Acc: 0.6239, Test Acc: 0.7064\n",
            "Epoch: 113, Train Loss: 0.0040, Val Acc: 0.5780, Test Acc: 0.7064\n",
            "Epoch: 114, Train Loss: 0.0052, Val Acc: 0.6239, Test Acc: 0.7064\n",
            "Epoch: 115, Train Loss: 0.0037, Val Acc: 0.6330, Test Acc: 0.7064\n",
            "Epoch: 116, Train Loss: 0.0030, Val Acc: 0.6147, Test Acc: 0.7064\n",
            "Epoch: 117, Train Loss: 0.0028, Val Acc: 0.6147, Test Acc: 0.7064\n",
            "Epoch: 118, Train Loss: 0.0027, Val Acc: 0.6055, Test Acc: 0.7064\n",
            "Epoch: 119, Train Loss: 0.0026, Val Acc: 0.6055, Test Acc: 0.7064\n",
            "Epoch: 120, Train Loss: 0.0026, Val Acc: 0.6147, Test Acc: 0.7064\n",
            "Epoch: 121, Train Loss: 0.0028, Val Acc: 0.5596, Test Acc: 0.7064\n",
            "Epoch: 122, Train Loss: 0.0038, Val Acc: 0.6239, Test Acc: 0.7064\n",
            "Epoch: 123, Train Loss: 0.0042, Val Acc: 0.5596, Test Acc: 0.7064\n",
            "Epoch: 124, Train Loss: 0.0043, Val Acc: 0.6055, Test Acc: 0.7064\n",
            "Epoch: 125, Train Loss: 0.0031, Val Acc: 0.6055, Test Acc: 0.7064\n",
            "Epoch: 126, Train Loss: 0.0027, Val Acc: 0.6147, Test Acc: 0.7064\n",
            "Epoch: 127, Train Loss: 0.0026, Val Acc: 0.6147, Test Acc: 0.7064\n",
            "Epoch: 128, Train Loss: 0.0025, Val Acc: 0.6147, Test Acc: 0.7064\n",
            "Epoch: 129, Train Loss: 0.0024, Val Acc: 0.6147, Test Acc: 0.7064\n",
            "Epoch: 130, Train Loss: 0.0024, Val Acc: 0.5963, Test Acc: 0.7064\n",
            "Epoch: 131, Train Loss: 0.0024, Val Acc: 0.5963, Test Acc: 0.7064\n",
            "Epoch: 132, Train Loss: 0.0025, Val Acc: 0.5688, Test Acc: 0.7064\n",
            "Epoch: 133, Train Loss: 0.0032, Val Acc: 0.6330, Test Acc: 0.7064\n",
            "Epoch: 134, Train Loss: 0.0071, Val Acc: 0.5596, Test Acc: 0.7064\n",
            "Epoch: 135, Train Loss: 0.0387, Val Acc: 0.5963, Test Acc: 0.7064\n",
            "Epoch: 136, Train Loss: 0.4986, Val Acc: 0.4679, Test Acc: 0.7064\n",
            "Epoch: 137, Train Loss: 0.4383, Val Acc: 0.5963, Test Acc: 0.7064\n",
            "Epoch: 138, Train Loss: 0.2912, Val Acc: 0.5046, Test Acc: 0.7064\n",
            "Epoch: 139, Train Loss: 0.2600, Val Acc: 0.5505, Test Acc: 0.7064\n",
            "Epoch: 140, Train Loss: 0.1919, Val Acc: 0.4954, Test Acc: 0.7064\n",
            "Epoch: 141, Train Loss: 0.1784, Val Acc: 0.5596, Test Acc: 0.7064\n",
            "Epoch: 142, Train Loss: 0.1567, Val Acc: 0.6055, Test Acc: 0.7064\n",
            "Epoch: 143, Train Loss: 0.1522, Val Acc: 0.6239, Test Acc: 0.7064\n",
            "Epoch: 144, Train Loss: 0.1147, Val Acc: 0.5413, Test Acc: 0.7064\n",
            "Epoch: 145, Train Loss: 0.0981, Val Acc: 0.5872, Test Acc: 0.7064\n",
            "Epoch: 146, Train Loss: 0.1315, Val Acc: 0.6055, Test Acc: 0.7064\n",
            "Epoch: 147, Train Loss: 0.1068, Val Acc: 0.6055, Test Acc: 0.7064\n",
            "Epoch: 148, Train Loss: 0.0728, Val Acc: 0.5780, Test Acc: 0.7064\n",
            "Epoch: 149, Train Loss: 0.0708, Val Acc: 0.5963, Test Acc: 0.7064\n",
            "Epoch: 150, Train Loss: 0.0641, Val Acc: 0.5596, Test Acc: 0.7064\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "sXEYMdV3rYlv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}